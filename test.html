<h1 class="unnumbered" id="global-objective">Global Objective</h1>
<p>The prediction and simulation of fluid partial differential equations
(PDEs) are inherently challenging due to the nonlinear nature of the
simulations and their design. In this study, we demonstrate the
application of PINNs to the 1D Burgers’ PDE to further explore the topic
of neural networks in the context of physics-informed predictions.</p>
<p>Direct solving of the PDE is compared and given to the resolution
methods of lines proposed by Biazar et al. <span class="citation"
data-cites="BiazarBERGER"></span>. The accuracy and efficiency are
directly compared to the feasibility and prediction of the PINN <span
class="citation" data-cites="MENG2023116172"></span>. The direct
solution is compared and estimated from the residuals of the epochs of
the machine learning process, and the data is feed-forwarded from the
PDE residuals to epochs. The final prediction for the 1D Burgers’
equation is solved and compared to the exact solution.</p>
<p><span class="math display">$$\begin{gathered}
    {\displaystyle {\frac {\partial u}{\partial t}}+u{\frac {\partial
u}{\partial x}}=\nu {\frac {\partial ^{2}u}{\partial x^{2}}}.}
\end{gathered}$$</span> Derived by Harry Bateman <span class="citation"
data-cites="harryBateman1915"></span>, the solution for <span
class="math inline"><em>f</em><sup>+</sup> = 2, <em>f</em><sup>−</sup> = 0, <em>c</em> = 1</span>
results to the following analytical solution to the PDE.</p>
<p><span class="math display">$${\displaystyle u(x,t)={\frac
{2}{1+e^{\frac {x-t}{\nu }}}}}$$</span></p>
<h1 id="pre-processing-strategem">Pre-processing strategem</h1>
<h2 id="standardization">Standardization</h2>
<p>Standardize the distribution of data to a mean of zero and standard
deviation of 1. Improves the model learning speed, alongside
distributing equal importance to all features.</p>
<h2 id="normalization">Normalization</h2>
<p>Normalize the data between 0 and 1, allowing the features to not
disproportionately affect the model in bias.</p>
<h2 id="sampling-strategies">Sampling Strategies</h2>
<p>Uniform grid is used to avoid sensitivity comparisons of non-uniform
bias grids.</p>
<h1 id="models">Models</h1>
<h2 id="standard-unsupervised-pinn">Standard unsupervised PINN</h2>
<p>In unsupervised learning, the CNN of the model is based on PDE’s
without labeled data (initial and boundary conditions for CFD
applications). The loss is only derived from the residuals to base a
solution on learned behaviors, eg. input-output pairs. <span
class="math display">$$\mathcal{L}_{\text{PDE}} = \frac{1}{M}
\sum_{j=1}^{M} \left( u_t(x_j, t_j) + u(x_j, t_j) u_x(x_j, t_j) - \nu
u_{xx}(x_j, t_j) \right)^2$$</span> <span
class="math display">ℒ = ℒ<sub>PDE</sub></span></p>
<h2 id="standard-supervised-pinn"> Standard supervised PINN</h2>
<p>In supervised learning, the CNN of the model is based on labeled
data, and when coupled with the residual function, the model will fit
and form properly.</p>
<p><span class="math display">$$\mathcal{L}_{\text{data}} = \frac{1}{N}
\sum_{i=1}^{N} \left( u_{\text{pred}}(x_i, t_i) - u_{\text{data}}(x_i,
t_i) \right)^2$$</span> K is thhe number of boundary condition points,
which are in the beginning and specified. M is the number of collocation
points specified for the PDE. N is the number of data points</p>
<p><span
class="math display">ℒ = ℒ<sub>data</sub> + ℒ<sub>PDE</sub></span></p>
<h2 id="supervised-extended-pinn">Supervised Extended PINN</h2>
<p>The loss function is a combination of the data loss (difference
between predictions and labeled data) and the PDE residuals: <span
class="math display">$$\mathcal{L}_{\text{BC}} = \frac{1}{K}
\sum_{k=1}^{K} \left( u_{\text{pred}}(x_{b_k}, t_{b_k}) - u_{b_k}
\right)^2$$</span> <span
class="math display">ℒ = ℒ<sub>data</sub> + ℒ<sub>PDE</sub> + ℒ<sub>BC</sub></span></p>
<h1 id="validation-and-verification">Validation and Verification</h1>
<p><br />
Comparison with Analytical Solutions - Available from Harry Bateman
<span class="citation" data-cites="harryBateman1915"></span>.</p>
<p>Sampling and Convergence Analysis - Collocation is important to
capture regions of high complexity such as regions where significant
changes or features in the solution occur.</p>
<p>Sensitivity Analysis - With regards to PINN this includes how number
of layers and neurons effect the accuracy and convergence rate.</p>
<h1 id="expected-results">Expected Results</h1>
<p>The expected results of using a PINN to solve the Burgers equation
are as follows: Training should be smooth, showing the model is learning
well. The model should focus more on complex areas without needing too
much computation. The model handles variations in initial and boundary
conditions, balancing accuracy and computation.</p>
<h1 id="goals">Goals</h1>
<p><strong>1. Analysis of Burgers Equation Using PINN:</strong> Analyze
and reconstruct the flow field of the Burgers equation using both
supervised and unsupervised PINNs.</p>
<p><strong>2. Accurate Solution Capture:</strong>Ensure the PINN model
accurately replicates the analytical solutions of the Burgers
equation</p>
<p><strong>3. Effective Sampling:</strong> Use smart sampling
techniques, such as adaptive and weighted sampling, to focus on complex
regions with significant changes</p>
<p><strong>4. Smooth Convergence:</strong> Achieve smooth and efficient
training of the model, indicating effective learning and proper
convergence to the solution.</p>
<p><strong>5. Robustness to Variations:</strong> Ensure the model
remains accurate even with uncertainties in initial and boundary
conditions</p>
<p><strong>6. Optimized Training:</strong> Balance accuracy and
computational efficiency by optimizing the network architecture and
training parameters.</p>
<p><strong>7. Visualization and Comparison:</strong>Visualize the
flowfield solution, comparing it with analytical solutions to
differences within numerical simulation.</p>